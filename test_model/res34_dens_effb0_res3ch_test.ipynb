{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블\n",
    "\n",
    "# 전체 앙상블 예측 및 결과 저장 코드\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import (\n",
    "    resnet34, densenet121, efficientnet_b0,\n",
    "    ResNet34_Weights, DenseNet121_Weights, EfficientNet_B0_Weights\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# 설정\n",
    "SLICE_ROOT = \"/data1/lidc-idri/slices\"\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 라벨 추출\n",
    "def extract_label_from_filename(filename):\n",
    "    try:\n",
    "        score = int(filename.split(\"_\")[-1].replace(\".npy\", \"\"))\n",
    "        if score == 3: return None\n",
    "        return 1 if score >= 4 else 0\n",
    "    except: return None\n",
    "\n",
    "# 데이터셋 정의\n",
    "class LIDC1chDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels): self.file_paths, self.labels = file_paths, labels\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        img = np.load(path).astype(np.float32)\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        img_tensor = torch.tensor(img)\n",
    "        img_tensor = F.interpolate(img_tensor.unsqueeze(0), size=(224, 224), mode='bilinear').squeeze(0)\n",
    "        return img_tensor, torch.tensor(self.labels[idx]).float(), path\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "\n",
    "class LIDC3chDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels): self.file_paths, self.labels = file_paths, labels\n",
    "    def __getitem__(self, idx):\n",
    "        center_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        folder = os.path.dirname(center_path)\n",
    "        fname = os.path.basename(center_path)\n",
    "        slice_num = int(fname.split(\"_\")[1])\n",
    "        suffix = fname.split(\"_\")[-1]\n",
    "        images = []\n",
    "        for sn in [slice_num-1, slice_num, slice_num+1]:\n",
    "            path = os.path.join(folder, f\"slice_{sn:03d}_{suffix}\")\n",
    "            img = np.load(path).astype(np.float32) if os.path.exists(path) else np.load(center_path).astype(np.float32)\n",
    "            img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "            images.append(img)\n",
    "        stacked = np.stack(images, axis=0)\n",
    "        img_tensor = torch.tensor(stacked)\n",
    "        img_tensor = F.interpolate(img_tensor.unsqueeze(0), size=(224,224), mode='bilinear').squeeze(0)\n",
    "        return img_tensor, torch.tensor(label).float(), center_path\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "\n",
    "# 데이터 로딩\n",
    "all_files = glob(os.path.join(SLICE_ROOT, \"LIDC-IDRI-*\", \"*.npy\"))\n",
    "file_label_pairs = [(f, extract_label_from_filename(f)) for f in all_files]\n",
    "file_label_pairs = [(f, l) for f, l in file_label_pairs if l is not None]\n",
    "files, labels = zip(*file_label_pairs)\n",
    "_, val_files, _, val_labels = train_test_split(files, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "val_loader_1ch = DataLoader(LIDC1chDataset(val_files, val_labels), batch_size=BATCH_SIZE)\n",
    "val_loader_3ch = DataLoader(LIDC3chDataset(val_files, val_labels), batch_size=BATCH_SIZE)\n",
    "\n",
    "# 모델 로딩\n",
    "def load_models():\n",
    "    m1 = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "    m1.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    m1.fc = nn.Linear(m1.fc.in_features, 1)\n",
    "    m1.load_state_dict(torch.load(\"best_model_resnet34.pth\"))\n",
    "    m1.to(DEVICE).eval()\n",
    "\n",
    "    m2 = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "    m2.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    m2.classifier = nn.Linear(m2.classifier.in_features, 1)\n",
    "    m2.load_state_dict(torch.load(\"best_model_densenet121.pth\"))\n",
    "    m2.to(DEVICE).eval()\n",
    "\n",
    "    m3 = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    m3.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    m3.classifier[1] = nn.Linear(m3.classifier[1].in_features, 1)\n",
    "    m3.load_state_dict(torch.load(\"best_model_efficientnet_b0.pth\"))\n",
    "    m3.to(DEVICE).eval()\n",
    "\n",
    "    m4 = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "    m4.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    m4.fc = nn.Linear(m4.fc.in_features, 1)\n",
    "    m4.load_state_dict(torch.load(\"best_model_resnet34_3ch.pth\"))\n",
    "    m4.to(DEVICE).eval()\n",
    "    return m1, m2, m3, m4\n",
    "\n",
    "resnet34_1ch, densenet, effnet, resnet34_3ch = load_models()\n",
    "\n",
    "# 예측 및 저장\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for (data1, data3) in zip(val_loader_1ch, val_loader_3ch):\n",
    "        x1, labels, paths = data1\n",
    "        x3, _, _ = data3\n",
    "        x1, x3, labels = x1.to(DEVICE), x3.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        prob1 = torch.sigmoid(resnet34_1ch(x1).squeeze())\n",
    "        prob2 = torch.sigmoid(densenet(x1).squeeze())\n",
    "        prob3 = torch.sigmoid(effnet(x1).squeeze())\n",
    "        prob4 = torch.sigmoid(resnet34_3ch(x3).squeeze())\n",
    "\n",
    "        ensemble_prob = (prob1 + prob2 + prob3 + prob4) / 4\n",
    "        ensemble_pred = (ensemble_prob > 0.5).long()\n",
    "\n",
    "        for i in range(len(paths)):\n",
    "            results.append({\n",
    "                \"file\": os.path.basename(paths[i]),\n",
    "                \"label\": int(labels[i].item()),\n",
    "                \"ensemble_pred\": int(ensemble_pred[i].item()),\n",
    "                \"resnet34_1ch_prob\": float(prob1[i]),\n",
    "                \"densenet121_prob\": float(prob2[i]),\n",
    "                \"efficientnet_b0_prob\": float(prob3[i]),\n",
    "                \"resnet34_3ch_prob\": float(prob4[i]),\n",
    "                \"ensemble_prob\": float(ensemble_prob[i])\n",
    "            })\n",
    "\n",
    "# CSV 저장\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"weighted_ensemble_predictions_expanded.csv\", index=False)\n",
    "\n",
    "# (선택) 성능 요약 출력\n",
    "print(\"\\n[Classification Report: Ensemble with expanded output]\")\n",
    "print(classification_report([r['label'] for r in results], [r['ensemble_pred'] for r in results], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. 더미 모델 정의\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(8, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# 2. 더미 데이터셋\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, channels):\n",
    "        self.data = [torch.rand(channels, 224, 224) for _ in range(6)]\n",
    "        self.labels = [1, 0, 1, 1, 0, 0]\n",
    "        self.paths = [f\"slice_{i:03d}_{s}.npy\" for i, s in enumerate(self.labels)]\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], torch.tensor(self.labels[idx]).float(), self.paths[idx]\n",
    "\n",
    "# 3. 장치 설정\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 4. 모델 준비\n",
    "resnet34_1ch = SimpleModel(1).to(DEVICE).eval()\n",
    "densenet      = SimpleModel(1).to(DEVICE).eval()\n",
    "effnet        = SimpleModel(1).to(DEVICE).eval()\n",
    "resnet34_3ch  = SimpleModel(3).to(DEVICE).eval()\n",
    "\n",
    "# 5. 데이터 로더\n",
    "val_loader_1ch = DataLoader(DummyDataset(1), batch_size=2)\n",
    "val_loader_3ch = DataLoader(DummyDataset(3), batch_size=2)\n",
    "\n",
    "# 6. 예측 및 결과 저장\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for (data1, data3) in zip(val_loader_1ch, val_loader_3ch):\n",
    "        x1, labels, paths = data1\n",
    "        x3, _, _ = data3\n",
    "        x1, x3, labels = x1.to(DEVICE), x3.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        p1 = torch.sigmoid(resnet34_1ch(x1).squeeze())\n",
    "        p2 = torch.sigmoid(densenet(x1).squeeze())\n",
    "        p3 = torch.sigmoid(effnet(x1).squeeze())\n",
    "        p4 = torch.sigmoid(resnet34_3ch(x3).squeeze())\n",
    "\n",
    "        p_ens = (p1 + p2 + p3 + p4) / 4\n",
    "        pred_ens = (p_ens > 0.5).long()\n",
    "\n",
    "        for i in range(len(paths)):\n",
    "            results.append({\n",
    "                \"file\": paths[i],\n",
    "                \"label\": int(labels[i]),\n",
    "                \"ensemble_pred\": int(pred_ens[i]),\n",
    "                \"resnet34_1ch_prob\": float(p1[i]),\n",
    "                \"densenet121_prob\": float(p2[i]),\n",
    "                \"efficientnet_b0_prob\": float(p3[i]),\n",
    "                \"resnet34_3ch_prob\": float(p4[i]),\n",
    "                \"ensemble_prob\": float(p_ens[i])\n",
    "            })\n",
    "\n",
    "# 7. CSV 저장\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"test_weighted_ensemble_predictions.csv\", index=False)\n",
    "print(\"✅ 저장 완료: test_weighted_ensemble_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lungcancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
