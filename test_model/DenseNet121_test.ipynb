{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Device: cuda:1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 1 has a total capacity of 11.72 GiB of which 1.38 MiB is free. Process 2782 has 208.00 MiB memory in use. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Process 1641829 has 4.22 GiB memory in use. Process 1689802 has 3.49 GiB memory in use. Of the allocated memory 3.49 GiB is allocated by PyTorch, and 93.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m model.features.conv0 = nn.Conv2d(\u001b[32m1\u001b[39m, \u001b[32m64\u001b[39m, kernel_size=\u001b[32m7\u001b[39m, stride=\u001b[32m2\u001b[39m, padding=\u001b[32m3\u001b[39m, bias=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     99\u001b[39m model.classifier = nn.Linear(model.classifier.in_features, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m criterion = nn.BCEWithLogitsLoss()\n\u001b[32m    103\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 915 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 1 has a total capacity of 11.72 GiB of which 1.38 MiB is free. Process 2782 has 208.00 MiB memory in use. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Process 1641829 has 4.22 GiB memory in use. Process 1689802 has 3.49 GiB memory in use. Of the allocated memory 3.49 GiB is allocated by PyTorch, and 93.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "\n",
    "# --- 설정 ---\n",
    "slice_root = \"/data1/lidc-idri/slices\"\n",
    "bbox_csv_path = \"/home/iujeong/lung_cancer/csv/allbb_noPoly.csv\"\n",
    "save_path = \"/home/iujeong/lung_cancer/pth\"\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Device: {device}\")\n",
    "\n",
    "# --- 데이터 증강 Transform ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# --- 라벨 추출 ---\n",
    "def extract_label_from_filename(filename):\n",
    "    try:\n",
    "        score = int(filename.split(\"_\")[-1].replace(\".npy\", \"\"))\n",
    "        if score == 3:\n",
    "            return None\n",
    "        return 1 if score >= 4 else 0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- 파일 로딩 및 3-way split ---\n",
    "all_files = glob(os.path.join(slice_root, \"LIDC-IDRI-*\", \"*.npy\"))\n",
    "file_label_pairs = [(f, extract_label_from_filename(f)) for f in all_files]\n",
    "file_label_pairs = [(f, l) for f, l in file_label_pairs if l is not None]\n",
    "files, labels = zip(*file_label_pairs)\n",
    "\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(files, labels, test_size=0.3, random_state=42)\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# --- Dataset 정의 ---\n",
    "class LIDCDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = np.load(file_path).astype(np.float32)\n",
    "        img = np.clip(img, -1000, 400)\n",
    "        img = (img + 1000) / 1400.0\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.expand_dims(img, axis=-1)  # (H, W, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.tensor(img.transpose(2, 0, 1)).float()  # [C, H, W]\n",
    "\n",
    "        return img, torch.tensor(label).float()\n",
    "\n",
    "# --- DataLoader ---\n",
    "train_loader = DataLoader(LIDCDataset(train_files, train_labels, transform=train_transform), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(LIDCDataset(val_files, val_labels, transform=val_transform), batch_size=batch_size)\n",
    "test_loader = DataLoader(LIDCDataset(test_files, test_labels, transform=val_transform), batch_size=batch_size)\n",
    "\n",
    "# --- 모델 정의 ---\n",
    "model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "model.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# --- 모델 저장 경로 설정 ---\n",
    "base_dir = os.getcwd()\n",
    "save_dir = os.path.join(base_dir, \"pth\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(save_dir, \"aug_densenet121_1.pth\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# --- 학습 루프 ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"[Train] Epoch: {epoch+1} Loss: {train_loss / len(train_loader):.4f} Accuracy: {(correct/total)*100:.2f}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs).squeeze()\n",
    "            preds = (probs > 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            total += labels.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"✅ Best model saved!\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# --- 테스트 ---\n",
    "print(\"\\n📊 Test Set Evaluation (Best Model 기준):\")\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).squeeze()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).long()\n",
    "        y_probs.extend(probs.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = (np.array(y_pred) == np.array(y_true)).mean() * 100\n",
    "print(f\"✅ Test Accuracy: {test_acc:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "try:\n",
    "    auc_score = roc_auc_score(y_true, y_probs)\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"AUC 계산 실패: 클래스가 모두 있어야 합니다.\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "\n",
    "# --- 설정 ---\n",
    "slice_root = \"/data1/lidc-idri/slices\"\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Device: {device}\")\n",
    "\n",
    "# --- 데이터 증강 Transform ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# --- 라벨 추출 ---\n",
    "def extract_label_from_filename(filename):\n",
    "    try:\n",
    "        score = int(filename.split(\"_\")[-1].replace(\".npy\", \"\"))\n",
    "        if score == 3:\n",
    "            return None\n",
    "        return 1 if score >= 4 else 0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- 파일 로딩 및 3-way split ---\n",
    "all_files = glob(os.path.join(slice_root, \"LIDC-IDRI-*\", \"*.npy\"))\n",
    "file_label_pairs = [(f, extract_label_from_filename(f)) for f in all_files]\n",
    "file_label_pairs = [(f, l) for f, l in file_label_pairs if l is not None]\n",
    "files, labels = zip(*file_label_pairs)\n",
    "\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(files, labels, test_size=0.3, random_state=42)\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# --- Dataset 정의 ---\n",
    "class LIDCDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = np.load(file_path).astype(np.float32)\n",
    "        img = np.clip(img, -1000, 400)\n",
    "        img = (img + 1000) / 1400.0\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.expand_dims(img, axis=-1)  # (H, W, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.tensor(img.transpose(2, 0, 1)).float()  # [C, H, W]\n",
    "\n",
    "        return img, torch.tensor(label).float()\n",
    "\n",
    "# --- DataLoader ---\n",
    "train_loader = DataLoader(LIDCDataset(train_files, train_labels, transform=train_transform), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(LIDCDataset(val_files, val_labels, transform=val_transform), batch_size=batch_size)\n",
    "test_loader = DataLoader(LIDCDataset(test_files, test_labels, transform=val_transform), batch_size=batch_size)\n",
    "\n",
    "# --- 모델 정의 ---\n",
    "model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "model.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# --- 모델 저장 경로 설정 ---\n",
    "base_dir = os.getcwd()\n",
    "save_dir = os.path.join(base_dir, \"pth\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(save_dir, \"best_model_densenet121.pth\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# --- 학습 루프 ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"[Train] Epoch: {epoch+1} Loss: {train_loss / len(train_loader):.4f} Accuracy: {(correct/total)*100:.2f}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs).squeeze()\n",
    "            preds = (probs > 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            total += labels.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"✅ Best model saved!\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# --- 테스트 ---\n",
    "print(\"\\n📊 Test Set Evaluation (Best Model 기준):\")\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).squeeze()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).long()\n",
    "        y_probs.extend(probs.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = (np.array(y_pred) == np.array(y_true)).mean() * 100\n",
    "print(f\"✅ Test Accuracy: {test_acc:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "try:\n",
    "    auc_score = roc_auc_score(y_true, y_probs)\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"AUC 계산 실패: 클래스가 모두 있어야 합니다.\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 234/234 [00:24<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.5816\n",
      "Validation Accuracy: 0.7200\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 234/234 [00:23<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 0.4496\n",
      "Validation Accuracy: 0.7600\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 234/234 [00:22<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 0.3113\n",
      "Validation Accuracy: 0.8063\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 234/234 [00:22<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 0.2154\n",
      "Validation Accuracy: 0.8263\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 234/234 [00:22<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.1479\n",
      "Validation Accuracy: 0.8087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 234/234 [00:23<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 0.1147\n",
      "Validation Accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 234/234 [00:22<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Loss: 0.0931\n",
      "Validation Accuracy: 0.8475\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 234/234 [00:22<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Loss: 0.0884\n",
      "Validation Accuracy: 0.7662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 234/234 [00:22<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Loss: 0.0951\n",
      "Validation Accuracy: 0.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 234/234 [00:23<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Loss: 0.0800\n",
      "Validation Accuracy: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 234/234 [00:23<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Loss: 0.0547\n",
      "Validation Accuracy: 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 234/234 [00:22<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Loss: 0.0395\n",
      "Validation Accuracy: 0.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 234/234 [00:22<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Loss: 0.0443\n",
      "Validation Accuracy: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 234/234 [00:22<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Loss: 0.0345\n",
      "Validation Accuracy: 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 234/234 [00:23<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Loss: 0.0883\n",
      "Validation Accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 234/234 [00:23<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Loss: 0.0615\n",
      "Validation Accuracy: 0.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 234/234 [00:21<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Loss: 0.0854\n",
      "Validation Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 234/234 [00:22<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Loss: 0.0367\n",
      "Validation Accuracy: 0.8638\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 234/234 [00:23<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Loss: 0.0191\n",
      "Validation Accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 234/234 [00:22<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Loss: 0.0138\n",
      "Validation Accuracy: 0.8662\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 234/234 [00:21<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Loss: 0.0218\n",
      "Validation Accuracy: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 234/234 [00:22<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Loss: 0.0560\n",
      "Validation Accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 234/234 [00:23<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Loss: 0.0856\n",
      "Validation Accuracy: 0.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 234/234 [00:22<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Loss: 0.0294\n",
      "Validation Accuracy: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 234/234 [00:22<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Loss: 0.0126\n",
      "Validation Accuracy: 0.8662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 234/234 [00:22<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Loss: 0.0222\n",
      "Validation Accuracy: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 234/234 [00:23<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Loss: 0.0249\n",
      "Validation Accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 234/234 [00:23<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Loss: 0.0328\n",
      "Validation Accuracy: 0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 234/234 [00:22<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Loss: 0.0349\n",
      "Validation Accuracy: 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 234/234 [00:23<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Loss: 0.0301\n",
      "Validation Accuracy: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 234/234 [00:22<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31] Loss: 0.0812\n",
      "Validation Accuracy: 0.8263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 234/234 [00:23<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32] Loss: 0.0141\n",
      "Validation Accuracy: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 234/234 [00:22<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33] Loss: 0.0083\n",
      "Validation Accuracy: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 234/234 [00:22<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34] Loss: 0.0127\n",
      "Validation Accuracy: 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 234/234 [00:22<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35] Loss: 0.0171\n",
      "Validation Accuracy: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 234/234 [00:23<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36] Loss: 0.1259\n",
      "Validation Accuracy: 0.8263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 234/234 [00:22<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37] Loss: 0.0199\n",
      "Validation Accuracy: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 234/234 [00:23<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38] Loss: 0.0070\n",
      "Validation Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 234/234 [00:23<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39] Loss: 0.0239\n",
      "Validation Accuracy: 0.7675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 234/234 [00:23<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40] Loss: 0.0358\n",
      "Validation Accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 234/234 [00:21<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41] Loss: 0.0131\n",
      "Validation Accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 234/234 [00:22<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42] Loss: 0.0062\n",
      "Validation Accuracy: 0.8725\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 234/234 [00:22<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43] Loss: 0.0435\n",
      "Validation Accuracy: 0.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 234/234 [00:22<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44] Loss: 0.0575\n",
      "Validation Accuracy: 0.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 234/234 [00:21<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45] Loss: 0.0261\n",
      "Validation Accuracy: 0.8562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 234/234 [00:23<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46] Loss: 0.0072\n",
      "Validation Accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 234/234 [00:22<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47] Loss: 0.0062\n",
      "Validation Accuracy: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 234/234 [00:22<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48] Loss: 0.0042\n",
      "Validation Accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|██████████| 234/234 [00:22<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49] Loss: 0.0025\n",
      "Validation Accuracy: 0.8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|██████████| 234/234 [00:23<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 50] Loss: 0.0024\n",
      "Validation Accuracy: 0.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|██████████| 234/234 [00:22<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 51] Loss: 0.0031\n",
      "Validation Accuracy: 0.8738\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 234/234 [00:22<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 52] Loss: 0.0690\n",
      "Validation Accuracy: 0.8113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|██████████| 234/234 [00:22<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 53] Loss: 0.0557\n",
      "Validation Accuracy: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|██████████| 234/234 [00:22<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 54] Loss: 0.0240\n",
      "Validation Accuracy: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|██████████| 234/234 [00:23<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 55] Loss: 0.0319\n",
      "Validation Accuracy: 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|██████████| 234/234 [00:20<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 56] Loss: 0.0192\n",
      "Validation Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|██████████| 234/234 [00:19<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 57] Loss: 0.0051\n",
      "Validation Accuracy: 0.8562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100: 100%|██████████| 234/234 [00:19<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 58] Loss: 0.0031\n",
      "Validation Accuracy: 0.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100: 100%|██████████| 234/234 [00:20<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 59] Loss: 0.0038\n",
      "Validation Accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100: 100%|██████████| 234/234 [00:20<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 60] Loss: 0.0032\n",
      "Validation Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100: 100%|██████████| 234/234 [00:19<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 61] Loss: 0.0245\n",
      "Validation Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100: 100%|██████████| 234/234 [00:19<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 62] Loss: 0.0514\n",
      "Validation Accuracy: 0.8113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100: 100%|██████████| 234/234 [00:19<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 63] Loss: 0.0407\n",
      "Validation Accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100: 100%|██████████| 234/234 [00:19<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64] Loss: 0.0322\n",
      "Validation Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100: 100%|██████████| 234/234 [00:19<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 65] Loss: 0.0058\n",
      "Validation Accuracy: 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100: 100%|██████████| 234/234 [00:19<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 66] Loss: 0.0038\n",
      "Validation Accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100: 100%|██████████| 234/234 [00:19<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 67] Loss: 0.0031\n",
      "Validation Accuracy: 0.8638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100: 100%|██████████| 234/234 [00:18<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 68] Loss: 0.0025\n",
      "Validation Accuracy: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100: 100%|██████████| 234/234 [00:20<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 69] Loss: 0.0072\n",
      "Validation Accuracy: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100: 100%|██████████| 234/234 [00:19<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 70] Loss: 0.0498\n",
      "Validation Accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100: 100%|██████████| 234/234 [00:19<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 71] Loss: 0.0198\n",
      "Validation Accuracy: 0.8237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|██████████| 234/234 [00:19<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 72] Loss: 0.0479\n",
      "Validation Accuracy: 0.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100: 100%|██████████| 234/234 [00:18<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 73] Loss: 0.0640\n",
      "Validation Accuracy: 0.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100: 100%|██████████| 234/234 [00:19<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 74] Loss: 0.0345\n",
      "Validation Accuracy: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100: 100%|██████████| 234/234 [00:19<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 75] Loss: 0.0339\n",
      "Validation Accuracy: 0.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: 100%|██████████| 234/234 [00:19<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 76] Loss: 0.0097\n",
      "Validation Accuracy: 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100: 100%|██████████| 234/234 [00:20<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 77] Loss: 0.0067\n",
      "Validation Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100: 100%|██████████| 234/234 [00:18<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 78] Loss: 0.0033\n",
      "Validation Accuracy: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100: 100%|██████████| 234/234 [00:19<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 79] Loss: 0.0037\n",
      "Validation Accuracy: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100: 100%|██████████| 234/234 [00:19<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 80] Loss: 0.0052\n",
      "Validation Accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100: 100%|██████████| 234/234 [00:20<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 81] Loss: 0.0184\n",
      "Validation Accuracy: 0.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|██████████| 234/234 [00:19<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 82] Loss: 0.0127\n",
      "Validation Accuracy: 0.8337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100: 100%|██████████| 234/234 [00:18<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 83] Loss: 0.0164\n",
      "Validation Accuracy: 0.8137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100: 100%|██████████| 234/234 [00:19<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 84] Loss: 0.0349\n",
      "Validation Accuracy: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100: 100%|██████████| 234/234 [00:20<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 85] Loss: 0.0235\n",
      "Validation Accuracy: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100: 100%|██████████| 234/234 [00:20<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 86] Loss: 0.0182\n",
      "Validation Accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100: 100%|██████████| 234/234 [00:19<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 87] Loss: 0.0059\n",
      "Validation Accuracy: 0.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100: 100%|██████████| 234/234 [00:18<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 88] Loss: 0.0092\n",
      "Validation Accuracy: 0.8575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100: 100%|██████████| 234/234 [00:19<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 89] Loss: 0.0040\n",
      "Validation Accuracy: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100: 100%|██████████| 234/234 [00:19<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 90] Loss: 0.0023\n",
      "Validation Accuracy: 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100: 100%|██████████| 234/234 [00:20<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 91] Loss: 0.0023\n",
      "Validation Accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100: 100%|██████████| 234/234 [00:19<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 92] Loss: 0.0022\n",
      "Validation Accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100: 100%|██████████| 234/234 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 93] Loss: 0.0023\n",
      "Validation Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100: 100%|██████████| 234/234 [00:19<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 94] Loss: 0.0022\n",
      "Validation Accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100: 100%|██████████| 234/234 [00:20<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 95] Loss: 0.0021\n",
      "Validation Accuracy: 0.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100: 100%|██████████| 234/234 [00:19<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96] Loss: 0.1181\n",
      "Validation Accuracy: 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100: 100%|██████████| 234/234 [00:19<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 97] Loss: 0.0386\n",
      "Validation Accuracy: 0.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100: 100%|██████████| 234/234 [00:18<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 98] Loss: 0.0085\n",
      "Validation Accuracy: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100: 100%|██████████| 234/234 [00:19<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 99] Loss: 0.0037\n",
      "Validation Accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|██████████| 234/234 [00:19<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 100] Loss: 0.0024\n",
      "Validation Accuracy: 0.8475\n",
      "\n",
      "📊 Test Set Evaluation (Best Model 기준):\n",
      "✅ Test Accuracy: 86.25%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8313    0.7527    0.7901       275\n",
      "         1.0     0.8766    0.9200    0.8978       525\n",
      "\n",
      "    accuracy                         0.8625       800\n",
      "   macro avg     0.8540    0.8364    0.8439       800\n",
      "weighted avg     0.8610    0.8625    0.8607       800\n",
      "\n",
      "AUC: 0.9094\n",
      "Confusion Matrix:\n",
      "[[207  68]\n",
      " [ 42 483]]\n"
     ]
    }
   ],
   "source": [
    "# Densenet121 with 3-way split and evaluation metrics + memory cleanup\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "\n",
    "# --- 설정 ---\n",
    "SLICE_ROOT = \"/data1/lidc-idri/slices\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Device: {DEVICE}\")\n",
    "\n",
    "# --- 라벨 추출 ---\n",
    "def extract_label_from_filename(filename):\n",
    "    try:\n",
    "        score = int(filename.split(\"_\")[-1].replace(\".npy\", \"\"))\n",
    "        if score == 3:\n",
    "            return None\n",
    "        return 1 if score >= 4 else 0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- 파일 로딩 및 3-way split ---\n",
    "all_files = glob(os.path.join(SLICE_ROOT, \"LIDC-IDRI-*\", \"*.npy\"))\n",
    "file_label_pairs = [(f, extract_label_from_filename(f)) for f in all_files]\n",
    "file_label_pairs = [(f, l) for f, l in file_label_pairs if l is not None]\n",
    "files, labels = zip(*file_label_pairs)\n",
    "\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(files, labels, test_size=0.3, random_state=42)\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# --- Dataset 정의 ---\n",
    "class LIDCDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = np.load(file_path).astype(np.float32)\n",
    "        img = np.clip(img, -1000, 400)\n",
    "        img = (img + 1000) / 1400.0\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return torch.tensor(img).float(), torch.tensor(label).float()\n",
    "\n",
    "# --- DataLoader ---\n",
    "train_loader = DataLoader(LIDCDataset(train_files, train_labels), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(LIDCDataset(val_files, val_labels), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(LIDCDataset(test_files, test_labels), batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- 모델 정의 ---\n",
    "model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "model.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "save_path = os.path.join(os.path.dirname(os.getcwd()), \"pth\", \"best_model_densenet121.pth\")\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# --- 학습 루프 ---\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        images, labels = images.to(DEVICE), labels.unsqueeze(1).to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    torch.cuda.empty_cache()  # 메모리 정리\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs).squeeze()\n",
    "            preds = (probs > 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            total += labels.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"✅ Best model saved!\")\n",
    "\n",
    "    torch.cuda.empty_cache()  # 검증 이후 메모리 정리\n",
    "\n",
    "# --- 테스트 ---\n",
    "print(\"\\n📊 Test Set Evaluation (Best Model 기준):\")\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images).squeeze()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).long()\n",
    "        y_probs.extend(probs.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = (np.array(y_pred) == np.array(y_true)).mean() * 100\n",
    "print(f\"✅ Test Accuracy: {test_acc:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "try:\n",
    "    auc_score = roc_auc_score(y_true, y_probs)\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"AUC 계산 실패: 클래스가 모두 있어야 합니다.\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lungcancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
