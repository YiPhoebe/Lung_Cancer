{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 PID: 4098834\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"현재 PID: {os.getpid()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 63/1570 [00:07<02:57,  8.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 172\u001b[39m\n\u001b[32m    169\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📈 Mean IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(ious)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m imgs, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     imgs, masks = imgs.to(device), masks.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     loss = criterion(outputs, masks)\n\u001b[32m    136\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     d1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     d2 = \u001b[38;5;28mself\u001b[39m.down2(F.max_pool2d(d1, \u001b[32m2\u001b[39m))\n\u001b[32m     64\u001b[39m     d3 = \u001b[38;5;28mself\u001b[39m.down3(F.max_pool2d(d2, \u001b[32m2\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/torch/nn/modules/module.py:1756\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m     forward_call = (\u001b[38;5;28mself\u001b[39m._slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward)\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# ✅ 경량 UNet + MGA 버전 (병변 마스크 포함)\n",
    "import os, numpy as np, torch, gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import re\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 하이퍼파라미터\n",
    "slice_root = \"/data1/lidc-idri/slices\"\n",
    "bbox_csv_path = \"/home/iujeong/lung_cancer/csv/allbb_noPoly.csv\"\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 채널 수 줄인 UNet\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        filters = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.down1 = DoubleConv(in_channels, filters[0])\n",
    "        self.down2 = DoubleConv(filters[0], filters[1])\n",
    "        self.down3 = DoubleConv(filters[1], filters[2])\n",
    "        self.down4 = DoubleConv(filters[2], filters[3])\n",
    "        self.bottom = DoubleConv(filters[3], filters[4])\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(filters[4], filters[3], 2, stride=2)\n",
    "        self.conv4 = DoubleConv(filters[4], filters[3])\n",
    "        self.up3 = nn.ConvTranspose2d(filters[3], filters[2], 2, stride=2)\n",
    "        self.conv3 = DoubleConv(filters[3], filters[2])\n",
    "        self.up2 = nn.ConvTranspose2d(filters[2], filters[1], 2, stride=2)\n",
    "        self.conv2 = DoubleConv(filters[2], filters[1])\n",
    "        self.up1 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)\n",
    "        self.conv1 = DoubleConv(filters[1], filters[0])\n",
    "\n",
    "        self.final = nn.Conv2d(filters[0], out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(F.max_pool2d(d1, 2))\n",
    "        d3 = self.down3(F.max_pool2d(d2, 2))\n",
    "        d4 = self.down4(F.max_pool2d(d3, 2))\n",
    "        b = self.bottom(F.max_pool2d(d4, 2))\n",
    "\n",
    "        u4 = self.conv4(torch.cat([self.up4(b), d4], 1))\n",
    "        u3 = self.conv3(torch.cat([self.up3(u4), d3], 1))\n",
    "        u2 = self.conv2(torch.cat([self.up2(u3), d2], 1))\n",
    "        u1 = self.conv1(torch.cat([self.up1(u2), d1], 1))\n",
    "\n",
    "        return torch.sigmoid(self.final(u1))\n",
    "\n",
    "# 마스크 불러오기\n",
    "def load_bbox_csv(path):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(path)\n",
    "    bbox_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        pid = row['pid']\n",
    "        slice_num = int(re.findall(r'\\d+', str(row['slice']))[0])\n",
    "        fname = f\"{pid}_slice{slice_num:03d}.npy\"\n",
    "        bbox = eval(row['bb'])\n",
    "        bbox_dict.setdefault(fname, []).append(bbox)\n",
    "    return bbox_dict\n",
    "\n",
    "# Dataset\n",
    "class CTMaskDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_dict, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_dict = mask_dict\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        img = np.load(path)\n",
    "        img = np.clip(img, -1000, 400)\n",
    "        img = (img + 1000) / 1400.\n",
    "        img = cv2.resize(img, (224, 224)).astype(np.float32)\n",
    "\n",
    "        fname = os.path.basename(path)\n",
    "        mask = np.zeros((224, 224), dtype=np.float32)\n",
    "        if fname in self.mask_dict:\n",
    "            for x1, y1, x2, y2 in self.mask_dict[fname]:\n",
    "                mask[y1:y2, x1:x2] = 1.0\n",
    "\n",
    "        img, mask = torch.tensor(img).unsqueeze(0), torch.tensor(mask).unsqueeze(0)\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self): return len(self.image_paths)\n",
    "\n",
    "# 학습 루프\n",
    "def train():\n",
    "    image_files = glob(os.path.join(slice_root, \"LIDC-IDRI-*\", \"*.npy\"))\n",
    "    train_imgs, val_imgs = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "    mask_dict = load_bbox_csv(bbox_csv_path)\n",
    "    train_set = CTMaskDataset(train_imgs, mask_dict)\n",
    "    val_set = CTMaskDataset(val_imgs, mask_dict)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "    model = UNet().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_total = 0\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total += loss.item()\n",
    "\n",
    "        avg_train_loss = loss_total / len(train_loader)\n",
    "        print(f\"\\n📚 Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # -------------------- Validation --------------------\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_pixels = 0\n",
    "        correct_pixels = 0\n",
    "        ious = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                outputs = model(imgs)\n",
    "                preds = (outputs > 0.5).float()\n",
    "\n",
    "                # IoU 계산\n",
    "                intersection = (preds * masks).sum(dim=(1,2,3))\n",
    "                union = (preds + masks).clamp(0,1).sum(dim=(1,2,3)) - intersection\n",
    "                iou = (intersection / (union + 1e-6)).mean().item()\n",
    "                ious.append(iou)\n",
    "\n",
    "                loss = criterion(outputs, masks)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                correct_pixels += (preds == masks).sum().item()\n",
    "                total_pixels += torch.numel(masks)\n",
    "\n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        val_acc = correct_pixels / total_pixels\n",
    "\n",
    "        print(f\"📊 Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"📈 Mean IoU: {np.mean(ious):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lungcancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
