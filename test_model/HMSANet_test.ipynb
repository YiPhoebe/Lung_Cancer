{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 학습 대상 슬라이스 수: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 184\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    148\u001b[39m all_files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(f.replace(\u001b[33m\"\u001b[39m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_mask.npy\u001b[39m\u001b[33m\"\u001b[39m))]\n\u001b[32m    149\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m총 학습 대상 슬라이스 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m train_files, val_files = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m train_loader = DataLoader(CTLesionDataset(train_files), batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    153\u001b[39m val_loader = DataLoader(CTLesionDataset(val_files), batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2851\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2848\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2853\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lungcancer/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2481\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2478\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2485\u001b[39m     )\n\u001b[32m   2487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 환경 설정\n",
    "SLICE_ROOT = \"/data1/lidc-idri/slices\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Dataset ---\n",
    "class CTLesionDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.slice_paths = files\n",
    "        self.mask_paths = [p.replace(\".npy\", \"_mask.npy\") for p in self.slice_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.load(self.slice_paths[idx])\n",
    "        mask = np.load(self.mask_paths[idx])\n",
    "\n",
    "        img = np.clip(img, -1000, 400)\n",
    "        img = (img + 1000) / 1400\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8)).resize((224, 224))\n",
    "        mask = Image.fromarray((mask * 255).astype(np.uint8)).resize((224, 224))\n",
    "\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "        mask = np.array(mask).astype(np.float32) / 255.0\n",
    "\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        mask = torch.tensor(mask).unsqueeze(0)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "# --- Loss Functions ---\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = preds.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (preds * targets).sum()\n",
    "        return 1 - ((2. * intersection + self.smooth) /\n",
    "                    (preds.sum() + targets.sum() + self.smooth))\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        return self.bce(preds, targets) + self.dice(preds, targets)\n",
    "\n",
    "# --- Model ---\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.BatchNorm2d(F_int))\n",
    "        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.BatchNorm2d(F_int))\n",
    "        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid())\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class UpBlockWithAttention(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, skip_ch):\n",
    "        super().__init__()\n",
    "        self.att = AttentionGate(F_g=in_ch // 2, F_l=skip_ch, F_int=out_ch)\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
    "        self.conv = ConvBlock(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        skip = self.att(x, skip)\n",
    "        diffY = skip.size()[2] - x.size()[2]\n",
    "        diffX = skip.size()[3] - x.size()[3]\n",
    "        x = F.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class HMSANet_Attention(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_channels, 64)\n",
    "        self.enc2 = ConvBlock(64, 128)\n",
    "        self.enc3 = ConvBlock(128, 256)\n",
    "        self.enc4 = ConvBlock(256, 512)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.center = ConvBlock(512, 1024)\n",
    "        self.dec4 = UpBlockWithAttention(1024, 512, 512)\n",
    "        self.dec3 = UpBlockWithAttention(512, 256, 256)\n",
    "        self.dec2 = UpBlockWithAttention(256, 128, 128)\n",
    "        self.dec1 = UpBlockWithAttention(128, 64, 64)\n",
    "        self.out = nn.Conv2d(64, out_channels, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        center = self.center(self.pool(e4))\n",
    "        d4 = self.dec4(center, e4)\n",
    "        d3 = self.dec3(d4, e3)\n",
    "        d2 = self.dec2(d3, e2)\n",
    "        d1 = self.dec1(d2, e1)\n",
    "        out = self.out_act(self.out(d1))\n",
    "        return out\n",
    "\n",
    "# --- Training & Validation ---\n",
    "def train():\n",
    "    all_files = glob(os.path.join(SLICE_ROOT, \"LIDC-IDRI-*\", \"*.npy\"))\n",
    "    all_files = [f for f in all_files if not f.endswith(\"_mask.npy\")]\n",
    "    all_files = [f for f in all_files if os.path.exists(f.replace(\".npy\", \"_mask.npy\"))]\n",
    "    print(f\"총 학습 대상 슬라이스 수: {len(all_files)}\")\n",
    "\n",
    "    train_files, val_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(CTLesionDataset(train_files), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(CTLesionDataset(val_files), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = HMSANet_Attention().to(DEVICE)\n",
    "    criterion = BCEDiceLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train\"):\n",
    "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Train Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Val\"):\n",
    "                imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, masks)\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Val Epoch {epoch+1}, Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lungcancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
